{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Way - 1"
      ],
      "metadata": {
        "id": "7L-krbzZhcwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.randn(2,3)  # 2D tensor\n",
        "\n",
        "out = F.softmax(x, dim=1)  # Softmax along the 2nd axis (index 1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJOGrXj1hgen",
        "outputId": "48241690-1329-4d67-bef5-7de75b1899e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7019, 0.2185, 0.0796],\n",
            "        [0.3939, 0.1136, 0.4925]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Way - 2"
      ],
      "metadata": {
        "id": "1f0LWlzbkDVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_anydim(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute softmax over any dimension of an n-dimensional tensor from scratch.\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor.\n",
        "        dim (int): Dimension along which to apply softmax (default: last dimension).\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor with softmax applied along specified dimension.\n",
        "    \"\"\"\n",
        "    # Step 1: Max subtraction for numerical stability\n",
        "    max_vals, _ = x.max(dim=dim, keepdim=True)\n",
        "    x_stable = x - max_vals\n",
        "\n",
        "    # Step 2: Exponentiation\n",
        "    exps = torch.exp(x_stable)\n",
        "\n",
        "    # Step 3: Sum and divide\n",
        "    sum_exps = exps.sum(dim=dim, keepdim=True)\n",
        "    softmax = exps / sum_exps\n",
        "\n",
        "    return softmax"
      ],
      "metadata": {
        "id": "zCMNcuxuh4cV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_anydim(x, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5n67aPWh7YD",
        "outputId": "d6c93bfe-4231-4c65-8b39-8bcaf75030da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7019, 0.2185, 0.0796],\n",
              "        [0.3939, 0.1136, 0.4925]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYsZChO8jt8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}