{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# leaky relu activation"
      ],
      "metadata": {
        "id": "Uw315XrZPQTD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LeakyReLU Activation Function**\n",
        "\n",
        "**LeakyReLU (Leaky Rectified Linear Unit)** is an activation function used in neural networks, particularly in deep learning models. It aims to address the \"dying ReLU\" problem by allowing a small, non-zero gradient when the unit is not active (i.e., when the input is negative).\n",
        "\n",
        "### Formula\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "x, & \\text{if } x \\geq 0 \\\\\n",
        "\\alpha x, & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "- \\( x \\): Input to the activation function\n",
        "- \\( alpha \\): Small positive constant (commonly 0.01)\n",
        "\n",
        "### Properties\n",
        "\n",
        "- **Non-linearity**: Allows the network to learn complex patterns.\n",
        "- **Avoids dying ReLUs**: Small slope for negative inputs prevents neurons from becoming inactive.\n",
        "- **Computationally efficient**: Simple and fast to compute.\n"
      ],
      "metadata": {
        "id": "yukGAnXNOwQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuGR0GSILpxu",
        "outputId": "4487d2e8-aa0b-4c16-f0fa-1e3af59195d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[-0.0451,  0.2639],\n",
            "        [ 0.2314, -0.9714],\n",
            "        [-0.3198, -1.0708]])\n",
            "-------------------------------------------\n",
            "LeakyReLU (torch built-in):\n",
            " tensor([[-0.0005,  0.2639],\n",
            "        [ 0.2314, -0.0097],\n",
            "        [-0.0032, -0.0107]])\n",
            "-------------------------------------------\n",
            "LeakyReLU (manual computation):\n",
            " tensor([[-0.0005,  0.2639],\n",
            "        [ 0.2314, -0.0097],\n",
            "        [-0.0032, -0.0107]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a (3, 2) tensor with random normal values\n",
        "x = torch.randn(3, 2)\n",
        "\n",
        "# Define slope for negative values\n",
        "alpha = 0.01\n",
        "\n",
        "# Apply LeakyReLU using PyTorch built-in function\n",
        "y_builtin = nn.LeakyReLU(negative_slope=0.01)\n",
        "y_builtin = y_builtin(x)\n",
        "\n",
        "# Apply LeakyReLU manually\n",
        "y_manual = torch.where(x >= 0, x, alpha * x)\n",
        "\n",
        "# Print results\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"LeakyReLU (torch built-in):\\n\", y_builtin)\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"LeakyReLU (manual computation):\\n\", y_manual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnYD-PcGOEMQ"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}